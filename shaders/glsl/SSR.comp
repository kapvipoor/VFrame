#version 450

#extension GL_ARB_separate_shader_objects : enable
#extension GL_ARB_shading_language_420pack : enable
#extension GL_GOOGLE_include_directive : enable

#include "Common.h"

layout (local_size_x = THREAD_GROUP_SIZE_X, local_size_y = THREAD_GROUP_SIZE_Y) in;
void main()
{
    // This is the maximum distance we wish the reflected ray to be
    float maxDistance = g_Info.ssrMaxDistance;

    // Resolution ranges between 0-1. Where 1.0 is sampling the scene at full resolution
    // And 0.5 is sampling the scene at half resolution. If samppling at full resultion,
    // we iterate at a a pace of 1 pixel on the reflected ray when marching over it. If 0.5,
    // then we iterate at a apace of 2 pixels.
    float resolution = g_Info.ssrResolution;

    // These are number of steps we take to find the exact point of intersection in step 2,
    // when using the binary search method between no-hit and hit points.
    uint steps = uint(g_Info.ssrSteps);

    // This variable varies between having overlapping samples if too thick to having holes
    // between samples if too thin.
    float thickness = g_Info.ssrThickness;

    vec2 texSize = vec2(RENDER_RESOLUTION_X, RENDER_RESOLUTION_Y);
    vec2 texCoord = vec2(gl_GlobalInvocationID.xy + 0.5)/texSize;

    vec3 uv = vec3(0.0);

    // This is the primary position from where a reflected ray is going to be launched off
    // This is in view space
    //vec4 positionFrom = texture(sampler2D(g_RT_SampledImages[SAMPLE_POSITION], g_LinearSampler), texCoord);
    float sampleDepth = texture(sampler2D(g_RT_SampledImages[SAMPLE_PRIMARY_DEPTH], g_NearestSampler), texCoord).x;
	vec4 positionFrom = GetPositionfromDepth(texCoord, sampleDepth);
    
    // We are sampling the roughness map to not run SSR for pixels with max roughness values
    float roughness = imageLoad(g_RT_StorageImages[STORE_ROUGH_METAL], ivec2(gl_GlobalInvocationID.xy)).x;
    if ( positionFrom.w <= 0.0 || roughness == 1.0) 
    { 
         imageStore(g_RT_StorageImages[STORE_SS_REFLECTION], ivec2(gl_GlobalInvocationID.xy), vec4(uv, 0.0));
         return;
    }

    vec3 unitPositionFrom = normalize(positionFrom.xyz);
    vec3 normal = normalize(texture(sampler2D(g_RT_SampledImages[SAMPLE_NORMAL], g_LinearSampler), texCoord).xyz);
    vec3 pivot = normalize(reflect(unitPositionFrom.xyz, normal));

    // Before the ray marching begins, the tracking of end is at the beginning point
    vec4 positionTo = positionFrom;

    vec4 startView = vec4(positionFrom.xyz + (pivot * 0.0), 1.0f); 

    // end view could be out of the view space. Mea
    vec4 endView = vec4((positionFrom.xyz) + (pivot * maxDistance), 1.0f);
    
    vec4 startFrag = startView;
    startFrag = g_Info.camProj * startFrag; // from view to screen space
    startFrag.xyz /= startFrag.w; // from screen space to NDC. Z ranges here from (-1, 1) 
    startFrag.xy = (startFrag.xy * vec2(0.5, -0.5)) + 0.5; // from NDC to UV
    startFrag.xy *= texSize; // from UV to Fragment Coordinate - this ranges betweem 0 and resolution

    // End frag could also be under 0 and over resolution
    vec4 endFrag = endView;
    endFrag = g_Info.camProj * endFrag; // from view to screen space
    endFrag.xyz /= endFrag.w; // from screen space to NDC. Z ranges here from (-1, 1) 
    endFrag.xy = (endFrag.xy * vec2(0.5, -0.5)) + 0.5; // from NDC to UV
    endFrag.xy = clamp(endFrag.xy, vec2(0.0, 0.0), vec2(1.0, 1.0));
    endFrag.xy *= texSize; // from UV to Fragment Coordinate - this ranges betweem 0 and resolution

    if(endFrag.w < 0)
    {
        imageStore(g_RT_StorageImages[STORE_SS_REFLECTION], ivec2(gl_GlobalInvocationID.xy), vec4(uv, 0.0));
        return;
    }

    vec2 frag = startFrag.xy;
    uv.xy = frag/texSize;

    float deltaX = endFrag.x - startFrag.x; // deltaX is the number of pixels to cover along the X axis (assuming resolution is 1.0 - full)
    float deltaY = endFrag.y - startFrag.y; // deltaY is the number of pixels to cover along the Y axis (assuming reslution is 1.0 - full)
    float useX = abs(deltaX) >= abs(deltaY) ? 1.0 : 0.0; // useX if deltaX is greater
    float delta = mix(abs(deltaY), abs(deltaX), useX) * clamp(resolution, 0.0, 1.0); // delta is higher of the 2 deltas multiplied by resolution
    vec2 increment = vec2(deltaX, deltaY)/max(delta, 0.001); // increment is the number of iteration we will take on the reflection ray
    // Remember. if the resolution is 0.5, then delta is half of largest delta, meaning increment is 2x. Meaning, then we jump 2 pixels
    // in every iteration. Hence the number of samples we will take along the reflection ray is also halved. Hence, cheaper.

    float search0 = 0; // search 0 holds the last position on the reflection line that did not hit or missed
    float search1 = 0; // search 1 holds the current position that lies between [start, end]

    int hit0 = 0; // intersection during first pass
    int hit1 = 0; // intersection during second pass  

    float viewDistance = startView.z; // this is the distance of how far the current position is from the camera in view space
    float depth = thickness; // no idea why we are initializing the depth to thickness

    int i = 0;
    // we are looping the number of pixels between start and end point.
    // we are essentially ray marching along the reflection line in this loop
    for(i=0; i < int(delta); ++i)
    {
        // frag is value between 0 and texture resolution, and we have incremented it on X and Y along the reflection ray in screen space
        // Hence we are working with Fragment values and not position in view space.
        // At any given point, frag + increment can be out of [0, res]
        frag += increment;
        uv.xy = frag/texSize; // this is uv of the next position along the reflection line
        float sampleDepth = texture(sampler2D(g_RT_SampledImages[SAMPLE_PRIMARY_DEPTH], g_NearestSampler), uv.xy).x;
	    positionTo = GetPositionfromDepth(uv.xy, sampleDepth);

        // since search 1 holds the value between start and end, we update search 1 to hold the pixels jumped
        // divided by total number of pixels to jump
        search1 = mix((frag.y - startFrag.y)/deltaY, (frag.x - startFrag.x)/deltaX, useX);
        search1 = clamp(search1, 0.0, 1.0);

        // now get the view distance at this new point. 
        // There is some perspective correction magic going on here as the view distance to the next point from camera is not
        // just linear interpolation between the start and end point. 
        // Refer https://www.comp.nus.edu.sg/~lowkl/publications/lowk_persp_interp_techrep.pdf for perspective corrected 
        // interpolation
        // However this magic isnt working? It is creating sme ghosting artifacts
        viewDistance = (startView.z * endView.z)/mix(endView.z, startView.z, search1);
        
        // depth here is the difference between the computed distance of the next position (along the relection line) 
        // from the camera and the distance of the sampled point at the same fragment from the camera. If depth is greater
        // than zero, then it means that the sampled position is closer to the camera than the iterated position. Hence
        // there is POSSIBILITY of an intersection. Otherwise the sampled position is further away from the camera than the
        // iterated position, implying we are in the air at the moment, not intersecting anything.
        depth =  positionTo.z - viewDistance;

        // We are also checking if the depth is less than a prescribed thickness
        if(depth > 0 && depth < thickness)
        {
            // we have essentially hit an object during the first phase
            hit0 = 1;
            break;
        }
        else
        {
            // otherwise just update the last unsuccessful position and 
            // iterate to the next position along the reflection line
            search0 = search1;
        }
    }

    // Now initating Phase 2
    // We are going to search between current and previous missed point along the reflection line for an exact point of intersection
    // If there is no hit, then current point is the end of reflection line (remember max distance).
    // As mentioned, exactly in the middle (binary search)
    search1= search0 + ((search1-search0) / 2.0);
     
    // If we have already found a hit point, then we iterate the binary search so many times to find an intersection as close 
    // as possible. Remember, this is still within the thickness range.
    steps *= hit0;
    for(i = 0; i < steps; ++i)
    {
        // Linear interpolating to calulate a frag that is somewhere between start and end
        // and exactly between previous miss and current hit
        frag = mix(startFrag.xy, endFrag.xy, search1);
        uv.xy = frag/texSize;
        float sampleDepth = texture(sampler2D(g_RT_SampledImages[SAMPLE_PRIMARY_DEPTH], g_LinearSampler), uv.xy).x;
	    positionTo = GetPositionfromDepth(uv.xy, sampleDepth);
        
        // Remember perspective magic to calculate the distance from the camera to the new point
        // However this magic isnt working? It is creating sme ghosting artifacts
        viewDistance = (startView.z * endView.z)/mix(endView.z, startView.z, search1);
        
        depth = positionTo.z - viewDistance;

        if(depth > 0 && depth < thickness)
        {
            hit1 = 1;
            search1 = search0 + ((search1 - search0) / 2.0);
            break;
        }
        else
        {
            // iterate on the binary search
            float temp = search1;
            search1 = search1 + ((search1 - search0)/2.0);
            search0 = temp;
        }
    }

    // now that we either have a closest hit detected or we do have a hit at all,
    // lets vertify visibility
    float visibility = 1.0
        //* hit0
        * positionTo.w
        * (1 - roughness) // higher the roughness, lower reflection contribution
        * (1 - max(dot(-unitPositionFrom, pivot), 0)) // ensures reflection is under 90 degrees; with highest visibility at 0 degrees
        * (1 - clamp(depth/thickness, 0 , 1)) // ensures higher visibility for more accurate hits
        * (1 - clamp(length(positionTo - positionFrom)/maxDistance, 0 , 1)) // ensures higher visibility for shorter reflection distance. Provides gradual drop off
        * ((uv.x < 0 || uv.x > 1) ? 0 : 1 ) // why dont we check this during the steps iteration and early break, set visibility and exit?
        * ((uv.y < 0 || uv.y > 1) ? 0 : 1 ) // why dont we check this during the steps iteration and early break, set visibility and exit?
        ;
    
    uv.z = clamp(visibility, 0 , 1);
    vec3 reflectionColor = texture(sampler2D(g_RT_SampledImages[SAMPLE_PRIMARY_COLOR], g_LinearSampler), uv.xy).xyz;

    imageStore(g_RT_StorageImages[STORE_SS_REFLECTION], ivec2(gl_GlobalInvocationID.xy), vec4(reflectionColor, uv.z));
}

